import numpy as np
import cv2
from scipy.signal import windows
from scipy.fft import fft2, ifft2, fftshift, ifftshift
import os
from skimage.morphology import skeletonize
import matplotlib.pyplot as plt
import math
import scipy.ndimage as ndimage
import scipy.signal as signal
import skimage.morphology
import skimage.measure
import skimage.draw # For drawing circles in visualization

# -----------------------------------------------------------------------------
# New Minutiae Extraction Implementation (Provided by User - Renamed and wrapped)
# -----------------------------------------------------------------------------

class MinutiaeFeature(object):
    """Represents a single minutia with its location, orientation, and type."""
    def __init__(self, locX: int, locY: int, Orientation: float, Type: str):
        self.locX = locX
        self.locY = locY
        self.Orientation = Orientation
        self.Type = Type

class _CoreMinutiaeFeatureExtractor: # Renamed to private to avoid confusion with wrapper
    """
    Core Minutiae Feature Extractor, provided by user.
    Performs skeletonization, minutiae detection (termination, bifurcation),
    angle computation, and spurious minutiae removal.
    """
    def __init__(self, spurious_minutiae_thresh: int = 10):
        self._mask = None # Internal mask generated by this extractor
        self._skel = None # Internal skeleton generated by this extractor
        self.minutiaeTerm = None # Binary image of termination points
        self.minutiaeBif = None  # Binary image of bifurcation points
        self._spuriousMinutiaeThresh = spurious_minutiae_thresh

    @property
    def skeleton_image(self) -> np.ndarray:
        return self._skel

    @property
    def internal_mask(self) -> np.ndarray:
        """Mask generated internally by this extractor (e.g., convex hull)."""
        return self._mask

    @property
    def termination_points_image(self) -> np.ndarray:
        """Binary image indicating termination minutiae locations."""
        return self.minutiaeTerm

    @property
    def bifurcation_points_image(self) -> np.ndarray:
        """Binary image indicating bifurcation minutiae locations."""
        return self.minutiaeBif

    def __skeletonize(self, img: np.ndarray):
        # Assumes img is already binary (0 or 255)
        img_bool = img > 0 # Convert to boolean for skimage.skeletonize
        self._skel = skimage.morphology.skeletonize(img_bool)
        self._skel = np.uint8(self._skel) * 255
        # The mask generated here is the initial region of interest for this module
        self._mask = img.copy()

    def __computeAngle(self, block: np.ndarray, minutiaeType: str) -> list[float]:
        """
        Computes angles for minutiae within a local block.
        Note: This method computes angles internally. For MTCC, these are typically
        overridden by STFT-derived orientations later in the pipeline.
        """
        angle = []
        (blkRows, blkCols) = np.shape(block)
        CenterX, CenterY = (blkRows - 1) / 2, (blkCols - 1) / 2

        if (minutiaeType.lower() == 'termination'):
            sumVal = 0
            for i in range(blkRows):
                for j in range(blkCols):
                    if ((i == 0 or i == blkRows - 1 or j == 0 or j == blkCols - 1) and block[i][j] != 0):
                        angle.append(-math.degrees(math.atan2(i - CenterY, j - CenterX)))
                        sumVal += 1
                        if (sumVal > 1): # More than one ridge ending in the boundary of the block -> spurious
                            angle = [float('nan')] # Mark as NaN to be filtered
                            break
            # If after loop sumVal is not 1, it's also spurious or not a true termination
            if sumVal != 1:
                 angle = [float('nan')] # Mark as NaN
            return angle

        elif (minutiaeType.lower() == 'bifurcation'):
            # For bifurcation, the block is 3x3, so WindowSize is 1. Center is (1,1)
            # The sum of values in the 3x3 block (excluding center) should be 3 for a bifurcation.
            # Here, it checks the sum in the boundary
            sumVal = 0
            for i in range(blkRows):
                for j in range(blkCols):
                    if ((i == 0 or i == blkRows - 1 or j == 0 or j == blkCols - 1) and block[i][j] != 0):
                        angle.append(-math.degrees(math.atan2(i - CenterY, j - CenterX)))
                        sumVal += 1
            if (sumVal != 3): # If not 3 connections on the boundary -> spurious
                angle = [float('nan')] # Mark as NaN
            return angle
        return [float('nan')] # Default for unknown type

    def __getTerminationBifurcation(self):
        # Convert skeleton to boolean for processing
        self._skel = self._skel == 255
        (rows, cols) = self._skel.shape
        self.minutiaeTerm = np.zeros_like(self._skel, dtype=np.uint8)
        self.minutiaeBif = np.zeros_like(self._skel, dtype=np.uint8)

        for i in range(1, rows - 1):
            for j in range(1, cols - 1):
                if (self._skel[i][j] == 1):
                    block = self._skel[i - 1:i + 2, j - 1:j + 2]
                    block_val = np.sum(block) # Sum of 1s in 3x3 window (crossing number logic)
                    if (block_val == 2): # Termination (one connection to center pixel)
                        self.minutiaeTerm[i, j] = 1
                    elif (block_val == 4): # Bifurcation (three connections to center pixel)
                        self.minutiaeBif[i, j] = 1

        # This part applies morphological operations to the internal mask
        self._mask = skimage.morphology.convex_hull_image(self._mask > 0)
        self._mask = skimage.morphology.erosion(self._mask, skimage.morphology.square(5))
        # Filter minutiae points that fall outside this eroded mask
        self.minutiaeTerm = self.minutiaeTerm * np.uint8(self._mask)
        self.minutiaeBif = self.minutiaeBif * np.uint8(self._mask)


    def __removeSpuriousMinutiae(self, minutiaeList_props, img_shape):
        """
        Filters spurious minutiae based on distance.
        `minutiaeList_props` is a list of regionprops (each having 'centroid').
        Returns a binary image with filtered minutiae.
        """
        filtered_img = np.zeros(img_shape, dtype=np.uint8)
        
        # Convert regionprops to a more convenient list of dicts for processing
        minutiae_points = [{'centroid': prop.centroid, 'label': prop.label} for prop in minutiaeList_props]
        numPoints = len(minutiae_points)

        # Track which minutiae are spurious
        is_spurious = [False] * numPoints

        for i in range(numPoints):
            for j in range(i + 1, numPoints): # Compare each pair once
                (X1, Y1) = minutiae_points[i]['centroid']
                (X2, Y2) = minutiae_points[j]['centroid']

                dist = np.sqrt((X2 - X1)**2 + (Y2 - Y1)**2)
                if (dist < self._spuriousMinutiaeThresh):
                    is_spurious[i] = True
                    is_spurious[j] = True # Mark both as spurious if too close

        # Reconstruct filtered minutiae list based on the spurious flags
        final_minutiae_props = []
        for i in range(numPoints):
            if not is_spurious[i]:
                (X, Y) = np.int16(minutiae_points[i]['centroid'])
                filtered_img[int(X), int(Y)] = 1 # Mark valid minutiae
                final_minutiae_props.append(minutiaeList_props[i]) # Keep original prop for feature extraction

        return filtered_img, final_minutiae_props

    def __cleanMinutiae(self, img_shape):
        """Applies spurious minutiae removal to termination and bifurcation points."""
        
        # Process Termination points
        labeled_term = skimage.measure.label(self.minutiaeTerm, connectivity=2)
        rp_term = skimage.measure.regionprops(labeled_term)
        
        # Pass regionprops and get filtered binary image and indices
        filtered_term_img, rp_term_filtered = self.__removeSpuriousMinutiae(rp_term, img_shape)
        self.minutiaeTerm = filtered_term_img # Update binary image of terminations
        
        # The rp_term_filtered will be used in __performFeatureExtraction, not stored here.
        # This function updates the *binary images* of minutiae points.

        # Process Bifurcation points (Note: the original code didn't filter bifurcation
        # points here in __cleanMinutiae. Extending to bif. for completeness)
        labeled_bif = skimage.measure.label(self.minutiaeBif, connectivity=2)
        rp_bif = skimage.measure.regionprops(labeled_bif)
        
        filtered_bif_img, rp_bif_filtered = self.__removeSpuriousMinutiae(rp_bif, img_shape)
        self.minutiaeBif = filtered_bif_img # Update binary image of bifurcations


    def __performFeatureExtraction(self) -> tuple[list[MinutiaeFeature], list[MinutiaeFeature]]:
        """
        Extracts final MinutiaeFeature objects with angles.
        """
        FeaturesTerm = []
        # Re-label after cleaning to get properties of the filtered points
        labeled_term = skimage.measure.label(self.minutiaeTerm, connectivity=2)
        rp_term = skimage.measure.regionprops(labeled_term)

        WindowSizeTerm = 2  # 5x5 block for termination
        for i_prop in rp_term:
            (row, col) = np.int16(np.round(i_prop['centroid']))
            
            # Ensure block extraction is within bounds
            if (row - WindowSizeTerm < 0 or row + WindowSizeTerm + 1 > self._skel.shape[0] or
                col - WindowSizeTerm < 0 or col + WindowSizeTerm + 1 > self._skel.shape[1]):
                continue

            block = self._skel[row - WindowSizeTerm:row + WindowSizeTerm + 1,
                               col - WindowSizeTerm:col + WindowSizeTerm + 1]
            
            angle = self.__computeAngle(block, 'Termination')
            if len(angle) == 1 and not math.isnan(angle[0]):
                FeaturesTerm.append(MinutiaeFeature(row, col, angle[0], 'Termination'))


        FeaturesBif = []
        labeled_bif = skimage.measure.label(self.minutiaeBif, connectivity=2)
        rp_bif = skimage.measure.regionprops(labeled_bif)
        WindowSizeBif = 1  # 3x3 block for bifurcation
        for i_prop in rp_bif:
            (row, col) = np.int16(np.round(i_prop['centroid']))

            # Ensure block extraction is within bounds
            if (row - WindowSizeBif < 0 or row + WindowSizeBif + 1 > self._skel.shape[0] or
                col - WindowSizeBif < 0 or col + WindowSizeBif + 1 > self._skel.shape[1]):
                continue

            block = self._skel[row - WindowSizeBif:row + WindowSizeBif + 1,
                               col - WindowSizeBif:col + WindowSizeBif + 1]
            
            angle = self.__computeAngle(block, 'Bifurcation')
            if len(angle) == 3 and not any(math.isnan(a) for a in angle): # Ensure all 3 angles are valid
                FeaturesBif.append(MinutiaeFeature(row, col, np.mean(angle), 'Bifurcation'))
        return (FeaturesTerm, FeaturesBif)

    def extractMinutiaeFeatures(self, enhanced_binary_img: np.ndarray) -> tuple[list[MinutiaeFeature], list[MinutiaeFeature]]:
        """
        Main method to extract minutiae features from a binary enhanced image.
        """
        if enhanced_binary_img.ndim != 2:
            raise ValueError("Input image must be grayscale (2D numpy array).")

        # Convert to 0/255 if not already
        if np.max(enhanced_binary_img) <= 1:
            img_processed = (enhanced_binary_img * 255).astype(np.uint8)
        else:
            img_processed = enhanced_binary_img.astype(np.uint8)

        # Step 1: Skeletonize the image (creates self._skel and self._mask)
        self.__skeletonize(img_processed)
        
        # Step 2: Get termination and bifurcation points (updates self.minutiaeTerm, self.minutiaeBif, self._mask)
        self.__getTerminationBifurcation()

        # Step 3: Remove spurious minutiae (updates self.minutiaeTerm, self.minutiaeBif - are binary images now)
        self.__cleanMinutiae(img_processed.shape)

        # Step 4: Perform final feature extraction including angle computation
        features_term, features_bif = self.__performFeatureExtraction()
        return (features_term, features_bif)

# -----------------------------------------------------------------------------
# New Gabor Filter Implementation (Provided by User - Renamed and wrapped)
# -----------------------------------------------------------------------------

class _CoreFingerprintImageEnhancer: # Renamed to private to avoid confusion with wrapper
    """Core Fingerprint Enhancer Object, provided by user."""

    def __init__(
        self,
        ridge_segment_blksze=16,
        ridge_segment_thresh=0.1,
        gradient_sigma=1,
        block_sigma=7,
        orient_smooth_sigma=7,
        ridge_freq_blksze=38,
        ridge_freq_windsze=5,
        min_wave_length=5,
        max_wave_length=15,
        relative_scale_factor_x=0.65,
        relative_scale_factor_y=0.65,
        angle_inc=3.0,
        ridge_filter_thresh=-3,
    ):
        self.ridge_segment_blksze = ridge_segment_blksze
        self.ridge_segment_thresh = ridge_segment_thresh
        self.gradient_sigma = gradient_sigma
        self.block_sigma = block_sigma
        self.orient_smooth_sigma = orient_smooth_sigma
        self.ridge_freq_blksze = ridge_freq_blksze
        self.ridge_freq_windsze = ridge_freq_windsze
        self.min_wave_length = min_wave_length
        self.max_wave_length = max_wave_length
        self.relative_scale_factor_x = relative_scale_factor_x
        self.relative_scale_factor_y = relative_scale_factor_y
        self.angle_inc = angle_inc
        self.ridge_filter_thresh = ridge_filter_thresh

        self._mask = None
        self._normim = None
        self._orientim = None
        self._mean_freq = None
        self._median_freq = None
        self._freq = None
        self._freqim = None
        self._binim = None

    @property
    def mask(self):
        return self._mask

    @property
    def normalized_image(self):
        return self._normim

    @property
    def orientation_map(self):
        return self._orientim

    @property
    def frequency_map(self):
        return self._freq

    @property
    def binary_enhanced_image(self):
        return self._binim

    def __normalise(self, img: np.ndarray) -> np.ndarray:
        if np.std(img) == 0:
            print("Warning: Image standard deviation is 0. Cannot normalize. Returning zero array.")
            return np.zeros_like(img, dtype=np.float32)
        normed = (img - np.mean(img)) / (np.std(img))
        return normed

    def __ridge_segment(self, img: np.ndarray):
        rows, cols = img.shape
        normalized_im = self.__normalise(img)

        new_rows = int(self.ridge_segment_blksze * np.ceil((float(rows)) / (float(self.ridge_segment_blksze))))
        new_cols = int(self.ridge_segment_blksze * np.ceil((float(cols)) / (float(self.ridge_segment_blksze))))

        padded_img = np.zeros((new_rows, new_cols), dtype=normalized_im.dtype)
        stddevim = np.zeros((new_rows, new_cols), dtype=normalized_im.dtype)
        padded_img[0:rows, 0:cols] = normalized_im

        for i in range(0, new_rows, self.ridge_segment_blksze):
            for j in range(0, new_cols, self.ridge_segment_blksze):
                block = padded_img[i : i + self.ridge_segment_blksze, j : j + self.ridge_segment_blksze]

                stddevim[i : i + self.ridge_segment_blksze, j : j + self.ridge_segment_blksze] = np.std(block) * np.ones(
                    block.shape
                )

        stddevim = stddevim[0:rows, 0:cols]
        self._mask = stddevim > self.ridge_segment_thresh
        mean_val = np.mean(normalized_im[self._mask]) if np.any(self._mask) else 0
        std_val = np.std(normalized_im[self._mask]) if np.any(self._mask) else 1
        if std_val == 0:
            std_val = 1e-6
        self._normim = (normalized_im - mean_val) / (std_val)

    def __ridge_orient(self) -> None:
        if self._normim is None:
            raise ValueError("Normalized image (_normim) is not set. Call __ridge_segment first.")

        rows, cols = self._normim.shape
        sze = int(np.fix(6 * self.gradient_sigma))
        if np.remainder(sze, 2) == 0:
            sze = sze + 1

        gauss = cv2.getGaussianKernel(sze, self.gradient_sigma)
        filter_gauss = gauss * gauss.T

        filter_grad_y, filter_grad_x = np.gradient(filter_gauss.astype(np.float32))

        gradient_x = signal.convolve2d(self._normim, filter_grad_x, mode="same", boundary='symm')
        gradient_y = signal.convolve2d(self._normim, filter_grad_y, mode="same", boundary='symm')

        grad_x2 = np.power(gradient_x, 2)
        grad_y2 = np.power(gradient_y, 2)
        grad_xy = gradient_x * gradient_y

        sze = int(np.fix(6 * self.block_sigma))
        gauss = cv2.getGaussianKernel(sze, self.block_sigma)
        filter_gauss = gauss * gauss.T

        grad_x2 = ndimage.convolve(grad_x2, filter_gauss, mode='nearest')
        grad_y2 = ndimage.convolve(grad_y2, filter_gauss, mode='nearest')
        grad_xy = 2 * ndimage.convolve(grad_xy, filter_gauss, mode='nearest')

        denom = np.sqrt(np.power(grad_xy, 2) + np.power((grad_x2 - grad_y2), 2)) + np.finfo(float).eps

        sin_2_theta = grad_xy / denom
        cos_2_theta = (grad_x2 - grad_y2) / denom

        if self.orient_smooth_sigma:
            sze = int(np.fix(6 * self.orient_smooth_sigma))
            if np.remainder(sze, 2) == 0:
                sze = sze + 1
            gauss = cv2.getGaussianKernel(sze, self.orient_smooth_sigma)
            filter_gauss = gauss * gauss.T
            cos_2_theta = ndimage.convolve(cos_2_theta, filter_gauss, mode='nearest')
            sin_2_theta = ndimage.convolve(sin_2_theta, filter_gauss, mode='nearest')

        self._orientim = np.pi / 2 + np.arctan2(sin_2_theta, cos_2_theta) / 2

    def __ridge_freq(self):
        if self._normim is None or self._orientim is None or self._mask is None:
            raise ValueError("Required maps (_normim, _orientim, _mask) are not set. Call __ridge_segment and __ridge_orient first.")

        rows, cols = self._normim.shape
        freq = np.zeros((rows, cols), dtype=np.float32)

        for i in range(0, rows - self.ridge_freq_blksze + 1, self.ridge_freq_blksze):
            for j in range(0, cols - self.ridge_freq_blksze + 1, self.ridge_freq_blksze):
                blkim = self._normim[i : i + self.ridge_freq_blksze, j : j + self.ridge_freq_blksze]
                blkor = self._orientim[i : i + self.ridge_freq_blksze, j : j + self.ridge_freq_blksze]
                
                block_mask = self._mask[i : i + self.ridge_freq_blksze, j : j + self.ridge_freq_blksze]
                if np.any(block_mask):
                    estimated_freq = self.__frequest(blkim, blkor)
                    freq[i : i + self.ridge_freq_blksze, j : j + self.ridge_freq_blksze] = estimated_freq
                else:
                    freq[i : i + self.ridge_freq_blksze, j : j + self.ridge_freq_blksze] = 0

        self._freq = freq * self._mask.astype(np.float32)
        freq_1d = self._freq.flatten()
        ind = np.where(freq_1d > 0)[0]

        if len(ind) > 0:
            non_zero_elems_in_freq = freq_1d[ind]
            self._mean_freq = np.mean(non_zero_elems_in_freq)
            self._median_freq = np.median(non_zero_elems_in_freq)
            self._freq = self._mean_freq * self._mask.astype(np.float32)
        else:
            self._mean_freq = 0.0
            self._median_freq = 0.0
            self._freq = np.zeros_like(self._freq)

    def __frequest(self, blkim: np.ndarray, blkor: np.ndarray) -> np.ndarray:
        rows, cols = np.shape(blkim)

        cosorient = np.mean(np.cos(2 * blkor))
        sinorient = np.mean(np.sin(2 * blkor))
        orient = math.atan2(sinorient, cosorient) / 2

        rotim = ndimage.rotate(blkim, orient / np.pi * 180 + 90, axes=(1, 0), reshape=False, order=3, mode="nearest")

        cropsze = int(np.fix(rows / np.sqrt(2)))
        offset = int(np.fix((rows - cropsze) / 2))
        
        if cropsze <= 0:
            return np.zeros(blkim.shape)
        
        rotim_cropped = rotim[offset : offset + cropsze, offset : offset + cropsze]

        if rotim_cropped.size == 0:
            return np.zeros(blkim.shape)

        proj = np.sum(rotim_cropped, axis=0)
        
        if len(proj) < self.ridge_freq_windsze:
            return np.zeros(blkim.shape)

        dilation = ndimage.grey_dilation(proj, size=self.ridge_freq_windsze, structure=np.ones(self.ridge_freq_windsze))

        temp = np.abs(dilation - proj)

        peak_thresh = 2

        maxpts = (temp < peak_thresh) & (proj > np.mean(proj))
        maxind = np.where(maxpts)[0]

        if len(maxind) < 2:
            return np.zeros(blkim.shape)

        no_of_peaks = len(maxind)
        wave_length = (maxind[no_of_peaks - 1] - maxind[0]) / (no_of_peaks - 1)

        if self.min_wave_length <= wave_length <= self.max_wave_length:
            return 1 / np.double(wave_length) * np.ones(blkim.shape)
        return np.zeros(blkim.shape)

    def __ridge_filter(self):
        if self._normim is None or self._orientim is None or self._freq is None or self._mask is None:
            raise ValueError("Required maps (_normim, _orientim, _freq, _mask) are not set. Call __ridge_segment, __ridge_orient, and __ridge_freq first.")

        norm_im = np.double(self._normim)
        rows, cols = norm_im.shape
        newim = np.zeros((rows, cols))

        freq_1d = self._freq.flatten()
        ind = np.where(freq_1d > 0)[0]

        if len(ind) == 0:
            self._binim = np.zeros_like(self._normim, dtype=bool)
            return

        non_zero_elems_in_freq = freq_1d[ind]
        non_zero_elems_in_freq = np.double(np.round((non_zero_elems_in_freq * 100))) / 100

        unfreq = np.unique(non_zero_elems_in_freq)

        if len(unfreq) == 0:
            fallback_freq = 1.0 / ((self.min_wave_length + self.max_wave_length) / 2.0)
            unfreq = np.array([fallback_freq])
            print("Warning: No valid frequencies detected for Gabor filter. Using fallback frequency.")

        base_freq = unfreq[0]
        sigmax = 1 / base_freq * self.relative_scale_factor_x
        sigmay = 1 / base_freq * self.relative_scale_factor_y

        sze = int(np.round(3 * np.max([sigmax, sigmay])))
        sze = max(1, sze)

        if sze % 2 == 0:
            sze += 1

        mesh_x, mesh_y = np.meshgrid(np.linspace(-sze, sze, (2 * sze + 1)), np.linspace(-sze, sze, (2 * sze + 1)))

        reffilter = np.exp(-(((np.power(mesh_x, 2)) / (sigmax * sigmax) + (np.power(mesh_y, 2)) / (sigmay * sigmay)))) * np.cos(
            2 * np.pi * base_freq * mesh_x
        )

        filt_rows, filt_cols = reffilter.shape

        angle_range = int(180 / self.angle_inc)
        if angle_range <= 0:
            angle_range = 1
            self.angle_inc = 180.0

        gabor_filter = np.array(np.zeros((angle_range, filt_rows, filt_cols)))

        for filter_idx in range(angle_range):
            rot_filt = ndimage.rotate(reffilter, -(filter_idx * self.angle_inc + 90), reshape=False, order=3, mode="nearest")
            gabor_filter[filter_idx] = rot_filt

        maxsze = int(sze)

        valid_pixels = self._mask & (self._freq > 0)
        validr, validc = np.where(valid_pixels)

        valid_indices = np.where(
            (validr >= maxsze) & (validr < rows - maxsze) &
            (validc >= maxsze) & (validc < cols - maxsze)
        )[0]

        validr = validr[valid_indices]
        validc = validc[valid_indices]

        maxorientindex = np.round(180 / self.angle_inc)
        orientindex = np.round(self._orientim / np.pi * 180 / self.angle_inc).astype(int)
        orientindex = np.mod(orientindex, angle_range)

        for k in range(len(validr)):
            cur_r = validr[k]
            cur_c = validc[k]

            r_start = cur_r - sze
            r_end = cur_r + sze + 1
            c_start = cur_c - sze
            c_end = cur_c + sze + 1

            r_start = max(0, r_start)
            r_end = min(rows, r_end)
            c_start = max(0, c_start)
            c_end = min(cols, c_end)

            img_block = norm_im[r_start:r_end, c_start:c_end]

            filter_slice_r_start = sze - (cur_r - r_start)
            filter_slice_r_end = filter_slice_r_start + (r_end - r_start)
            filter_slice_c_start = sze - (cur_c - c_start)
            filter_slice_c_end = filter_slice_c_start + (c_end - c_start) # Corrected typo c_c_start -> c_start

            current_gabor_kernel = gabor_filter[orientindex[cur_r, cur_c]] # Use already modded index
            
            current_gabor_kernel_cropped = current_gabor_kernel[
                filter_slice_r_start:filter_slice_r_end,
                filter_slice_c_start:filter_slice_c_end
            ]

            if img_block.shape == current_gabor_kernel_cropped.shape and img_block.size > 0:
                newim[cur_r, cur_c] = np.sum(img_block * current_gabor_kernel_cropped)
            else:
                newim[cur_r, cur_c] = 0

        self._binim = newim < self.ridge_filter_thresh

    def enhance(self, img: np.ndarray, resize: bool = False, invert_output=False) -> np.ndarray:
        if img.ndim != 2:
            raise ValueError("Input image must be grayscale (2D numpy array).")

        img_to_process = img.copy()

        if resize:
            rows, cols = np.shape(img_to_process)
            aspect_ratio = np.double(rows) / np.double(cols)

            new_rows = 350
            new_cols = int(new_rows / aspect_ratio)

            img_to_process = cv2.resize(img_to_process, (new_cols, new_rows))

        img_to_process = img_to_process.astype(np.float32)

        self.__ridge_segment(img_to_process)
        self.__ridge_orient()
        self.__ridge_freq()
        self.__ridge_filter()

        if self._binim is None:
            print("Warning: _ridge_filter did not produce a binary image. Returning zeros.")
            return np.zeros_like(img_to_process, dtype=bool)

        if invert_output:
            self._binim = ~self._binim
        return self._binim


# 0. Configuration / Parameters
# -----------------------------------------------------------------------------
class MTCCParameters:
    """
    Centralized configuration parameters for the MTCC fingerprint recognition pipeline.
    """
    def __init__(self):
        # MTCC specific parameters
        self.cylinder_radius = 65
        self.spatial_sectors = 18  # NS
        self.angular_sectors = 5   # ND
        self.gaussian_spatial = 6  # σS for spatial contribution
        self.gaussian_directional = np.pi / 10 # σD for angular/texture difference contribution
        self.gaussian_frequency = 2 # σF for frequency difference contribution
        self.gaussian_energy = 2    # σE for energy difference contribution

        # STFT specific parameters (for texture map generation)
        self.stft_window = 14
        self.stft_overlap = 6

        # Preprocessor parameters (for initial mask)
        self.segmentation_block_size = 16
        self.segmentation_kernel_size = (5,5)

        # Matcher parameters
        self.matcher_top_n_pairs = 20 # Number of top matching pairs to select in LSSR

        # Parameters for the new Gabor Enhancer (_CoreFingerprintImageEnhancer)
        self.ridge_segment_blksze = 16
        self.ridge_segment_thresh = 0.1
        self.gradient_sigma = 1
        self.block_sigma = 7
        self.orient_smooth_sigma = 7
        self.ridge_freq_blksze = 38
        self.ridge_freq_windsze = 5
        self.min_wave_length = 5
        self.max_wave_length = 15
        self.relative_scale_factor_x = 0.65
        self.relative_scale_factor_y = 0.65
        self.angle_inc = 3.0
        self.ridge_filter_thresh = -3

        # Parameters for the new Minutiae Extractor (_CoreMinutiaeFeatureExtractor)
        self.spurious_minutiae_thresh = 10


# -----------------------------------------------------------------------------
# 1. Core Preprocessing Pipeline Classes (Unchanged, except import removal)
# -----------------------------------------------------------------------------

class Preprocessor:
    """
    Loads fingerprint image and applies initial preprocessing including
    variance-based segmentation with morphological smoothing.
    This module's primary role is now to provide the initial `mask` for minutiae filtering
    and a `normalized_img` for STFT feature extraction, as the new Gabor enhancer
    performs its own normalization and segmentation internally.
    """
    def __init__(self, params: MTCCParameters):
        self.block_size = params.segmentation_block_size
        self.kernel = np.ones(params.segmentation_kernel_size, np.uint8)

    def process(self, image_path: str) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Processes a fingerprint image to perform segmentation and normalization.

        Args:
            image_path (str): Path to the input grayscale fingerprint image.

        Returns:
            tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing:
                - normalized_img_for_stft (np.ndarray): The intensity normalized image (float32).
                - mask (np.ndarray): The foreground mask of the fingerprint region (uint8, 0/255).
                - original_img (np.ndarray): The loaded original image (uint8, 0-255).

        Raises:
            FileNotFoundError: If the image at the specified path does not exist.
        """
        original_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if original_img is None:
            raise FileNotFoundError(f"Image not found at {image_path}")

        # Normalize intensity to 0-255 range (for consistent input)
        img_uint8 = cv2.normalize(original_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        
        # Calculate local standard deviation for variance-based segmentation
        img_float = img_uint8.astype(np.float32)
        mean_sq = cv2.boxFilter(img_float * img_float, -1, (self.block_size, self.block_size))
        mean_val = cv2.boxFilter(img_float, -1, (self.block_size, self.block_size))
        local_std = np.sqrt(np.maximum(0, mean_sq - mean_val**2))

        # Threshold local standard deviation to get a foreground mask
        mask = (local_std > np.mean(local_std) * 0.5).astype(np.uint8) * 255

        # Morphological smoothing of the mask
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, self.kernel, iterations=1)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, self.kernel, iterations=1)

        # The normalized image for STFT features.
        if np.any(mask == 255):
            mean_masked = np.mean(img_float[mask == 255])
            std_masked = np.std(img_float[mask == 255])
            if std_masked == 0:
                std_masked = 1e-6
            norm_img_for_stft = (img_float - mean_masked) / std_masked
        else:
            norm_img_for_stft = np.zeros_like(img_float)
        
        norm_img_for_stft[mask == 0] = 0

        return norm_img_for_stft, mask, original_img

class STFTFeatureExtractor:
    """
    Performs STFT-based feature extraction to obtain orientation, frequency,
    energy, and coherence maps. These maps are used for MTCC descriptor generation.
    """
    def __init__(self, params: MTCCParameters):
        self.window_size = params.stft_window
        self.overlap = params.stft_overlap
        self.step_size = self.window_size - self.overlap
        if self.step_size <= 0:
            self.step_size = 1
        self.spectral_window = np.outer(windows.cosine(self.window_size), windows.cosine(self.window_size))

    def process(self, image: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        h, w = image.shape

        orient_cos_accumulator = np.zeros_like(image, dtype=np.float32)
        orient_sin_accumulator = np.zeros_like(image, dtype=np.float32)
        freq_accumulator = np.zeros_like(image, dtype=np.float32)
        energy_accumulator = np.zeros_like(image, dtype=np.float32)
        feature_weights = np.zeros_like(image, dtype=np.float32)


        for y in range(0, h - self.window_size + 1, self.step_size):
            for x in range(0, w - self.window_size + 1, self.step_size):
                block = image[y:y+self.window_size, x:x+self.window_size].astype(np.float32)
                block_mean = np.mean(block)
                block_std = np.std(block)

                if block_std == 0:
                    continue

                block = (block - block_mean) / block_std
                
                windowed_block = block * self.spectral_window

                F = fftshift(fft2(windowed_block))
                magnitude_spectrum = np.abs(F)

                block_energy = np.log(np.sum(magnitude_spectrum**2) + 1e-10)

                if np.max(magnitude_spectrum) > 0:
                    dom_freq_idx = np.unravel_index(np.argmax(magnitude_spectrum), magnitude_spectrum.shape)
                    center_y, center_x = self.window_size // 2, self.window_size // 2
                    k_y = dom_freq_idx[0] - center_y
                    k_x = dom_freq_idx[1] - center_x

                    orientation_2theta = np.arctan2(k_y, k_x)
                    
                    orient_cos_accumulator[y:y+self.window_size, x:x+self.window_size] += np.cos(orientation_2theta)
                    orient_sin_accumulator[y:y+self.window_size, x:x+self.window_size] += np.sin(orientation_2theta)

                    frequency = np.sqrt(k_x**2 + k_y**2) / self.window_size
                    freq_accumulator[y:y+self.window_size, x:x+self.window_size] += frequency
                    energy_accumulator[y:y+self.window_size, x:x+self.window_size] += block_energy
                    feature_weights[y:y+self.window_size, x:x+self.window_size] += 1

        feature_weights[feature_weights == 0] = 1
        orientation_map = (np.arctan2(orient_sin_accumulator, orient_cos_accumulator) / 2.0 + np.pi/2) % np.pi
        frequency_map = freq_accumulator / feature_weights
        energy_map = energy_accumulator / feature_weights

        orientation_map = cv2.GaussianBlur(orientation_map, (5,5), 0)
        frequency_map = cv2.GaussianBlur(frequency_map, (5,5), 0)
        energy_map = cv2.GaussianBlur(energy_map, (5,5), 0)

        coherence_map = np.ones_like(image, dtype=np.float32) * 0.8 

        return orientation_map, frequency_map, energy_map, coherence_map

class EnhancedGaborFilterWrapper:
    """
    A wrapper class for the provided _CoreFingerprintImageEnhancer.
    It exposes the enhancement process and provides access to internally
    calculated maps (mask, normalized_image, orientation_map, frequency_map).
    """
    def __init__(self, params: MTCCParameters):
        self.enhancer_params = {
            'ridge_segment_blksze': params.ridge_segment_blksze,
            'ridge_segment_thresh': params.ridge_segment_thresh,
            'gradient_sigma': params.gradient_sigma,
            'block_sigma': params.block_sigma,
            'orient_smooth_sigma': params.orient_smooth_sigma,
            'ridge_freq_blksze': params.ridge_freq_blksze,
            'ridge_freq_windsze': params.ridge_freq_windsze,
            'min_wave_length': params.min_wave_length,
            'max_wave_length': params.max_wave_length,
            'relative_scale_factor_x': params.relative_scale_factor_x,
            'relative_scale_factor_y': params.relative_scale_factor_y,
            'angle_inc': params.angle_inc,
            'ridge_filter_thresh': params.ridge_filter_thresh,
        }
        self._enhancer_instance = None # To store the instance after process

    def process(self, image: np.ndarray, invert_output: bool = False) -> dict:
        self._enhancer_instance = _CoreFingerprintImageEnhancer(**self.enhancer_params)
        
        # The core enhancer expects raw image and returns binary (bool)
        enhanced_binary_bool = self._enhancer_instance.enhance(image.copy(), resize=False, invert_output=invert_output)

        return {
            'enhanced_binary_img': enhanced_binary_bool.astype(np.uint8) * 255,
            'mask': (self._enhancer_instance.mask * 255).astype(np.uint8) if self._enhancer_instance.mask is not None else np.zeros_like(image, dtype=np.uint8),
            'normalized_image': self._enhancer_instance.normalized_image if self._enhancer_instance.normalized_image is not None else np.zeros_like(image, dtype=np.float32),
            'orientation_map': self._enhancer_instance.orientation_map if self._enhancer_instance.orientation_map is not None else np.zeros_like(image, dtype=np.float32),
            'frequency_map': self._enhancer_instance.frequency_map if self._enhancer_instance.frequency_map is not None else np.zeros_like(image, dtype=np.float32),
        }

class TextureFeatureExtractor:
    """
    Extracts STFT-based texture features (orientation, frequency, energy maps).
    This class primarily serves as a logical grouping for the texture maps.
    """
    def process(self, orientation_map: np.ndarray, frequency_map: np.ndarray, energy_map: np.ndarray) -> dict:
        texture_maps = {
            'orientation': orientation_map, # Io
            'frequency': frequency_map,     # If
            'energy': energy_map            # Ie
        }
        return texture_maps

# -----------------------------------------------------------------------------
# 2. Minutiae Extraction and Quality Assessment Classes (New Wrapper)
# -----------------------------------------------------------------------------

class MinutiaeExtractor:
    """
    Wrapper for _CoreMinutiaeFeatureExtractor.
    Handles calling the core extraction logic and integrating with pipeline data.
    Overrides minutiae orientation with STFT-derived orientations.
    """
    def __init__(self, params: MTCCParameters):
        self.core_extractor = _CoreMinutiaeFeatureExtractor(spurious_minutiae_thresh=params.spurious_minutiae_thresh)
        # Removed: self.min_dist_sq = params.minutiae_min_distance_sq # This attribute is not needed here

    def process(self, enhanced_binary_img: np.ndarray, initial_mask: np.ndarray, stft_orientation_map: np.ndarray) -> dict:
        """
        Extracts minutiae using the wrapped core extractor and assigns STFT orientations.

        Args:
            enhanced_binary_img (np.ndarray): The enhanced, binary image (0 or 255).
            initial_mask (np.ndarray): The mask from the Preprocessor (0 or 255).
            stft_orientation_map (np.ndarray): The orientation map from STFT feature extraction.

        Returns:
            dict: Containing:
                - 'minutiae_list': List of minutiae (dict format for MTCC).
                - 'thinned_skeleton': The skeletonized image (from core extractor).
                - 'minutiaeTerm_img': Binary image of termination points (from core extractor).
                - 'minutiaeBif_img': Binary image of bifurcation points (from core extractor).
        """
        # Step 1: Use the core extractor to get raw minutiae features
        features_term_raw, features_bif_raw = self.core_extractor.extractMinutiaeFeatures(enhanced_binary_img)

        minutiae_list = []
        img_h, img_w = stft_orientation_map.shape # Use shape of orientation map for bounds check

        # Process termination minutiae
        for m_raw in features_term_raw:
            r, c = m_raw.locX, m_raw.locY
            # Ensure minutia is within valid region and image bounds
            if initial_mask[r, c] == 255 and (0 <= r < img_h and 0 <= c < img_w):
                # Get orientation from STFT map
                orientation_rad = stft_orientation_map[r, c]
                minutiae_list.append({
                    'x': c,
                    'y': r,
                    'type': 'ending',
                    'orientation': orientation_rad,
                    'quality': 1.0 # Placeholder, could derive from other maps
                })

        # Process bifurcation minutiae
        for m_raw in features_bif_raw:
            r, c = m_raw.locX, m_raw.locY
            # Ensure minutia is within valid region and image bounds
            if initial_mask[r, c] == 255 and (0 <= r < img_h and 0 <= c < img_w):
                orientation_rad = stft_orientation_map[r, c]
                minutiae_list.append({
                    'x': c,
                    'y': r,
                    'type': 'bifurcation',
                    'orientation': orientation_rad,
                    'quality': 1.0
                })

        return {
            'minutiae_list': minutiae_list, # The list of minutiae in MTCC expected format
            'thinned_skeleton': self.core_extractor.skeleton_image,
            'minutiaeTerm_img': self.core_extractor.termination_points_image * 255, # Convert bool to uint8 0/255
            'minutiaeBif_img': self.core_extractor.bifurcation_points_image * 255,   # Convert bool to uint8 0/255
            'enhancer_internal_mask_minutiae': self.core_extractor.internal_mask * 255 # Mask generated internally by new minutiae extractor
        }

# -----------------------------------------------------------------------------
# 3. MTCC Descriptor Generation Class (Unchanged, uses texture_maps)
# -----------------------------------------------------------------------------
class MTCCDescriptorGenerator:
    """
    Generates MTCC (Minutiae Texture Cylinder Codes) descriptors for each minutia.
    """
    def __init__(self, params: MTCCParameters):
        self.radius = params.cylinder_radius
        self.NS = params.spatial_sectors
        self.ND = params.angular_sectors
        self.sigma_s = params.gaussian_spatial
        self.sigma_d_orient = params.gaussian_directional
        self.sigma_d_freq = params.gaussian_frequency
        self.sigma_d_energy = params.gaussian_energy

    def _create_3d_cylinder(self, minutia: dict) -> dict:
        cylinder = {
            'center_minutia': minutia,
            'radius': self.radius,
            'spatial_sectors': self.NS,
            'angular_sectors': self.ND,
            'cells': []
        }

        delta_s = (2 * self.radius) / self.NS
        
        for s_idx in range(self.NS):
            for d_idx in range(self.ND):
                rel_x = -self.radius + s_idx * delta_s + delta_s / 2
                rel_y = -self.radius + d_idx * delta_s + delta_s / 2

                cos_theta = np.cos(minutia['orientation'])
                sin_theta = np.sin(minutia['orientation'])

                cell_center_x = minutia['x'] + (rel_x * cos_theta - rel_y * sin_theta)
                cell_center_y = minutia['y'] + (rel_x * sin_theta + rel_y * cos_theta)

                cylinder['cells'].append({
                    's_idx': s_idx,
                    'd_idx': d_idx,
                    'center_x': int(cell_center_x),
                    'center_y': int(cell_center_y),
                    'contributions': {}
                })
        return cylinder

    def _calculate_spatial_contribution(self, minutia: dict, neighbor_minutia: dict) -> float:
        dist = np.sqrt((minutia['x'] - neighbor_minutia['x'])**2 + \
                       (minutia['y'] - neighbor_minutia['y'])**2)
        return np.exp(-0.5 * (dist / self.sigma_s)**2)

    def _calculate_texture_contribution(self, value1: float, value2: float, sigma_d: float) -> float:
        diff = np.abs(value1 - value2)
        return np.exp(-0.5 * (diff / sigma_d)**2)

    def process(self, minutiae_list: list[dict], texture_maps: dict) -> list[dict]:
        mtcc_descriptors = []

        orientation_map = texture_maps['orientation']
        frequency_map = texture_maps['frequency']
        energy_map = texture_maps['energy']

        img_h, img_w = orientation_map.shape

        for central_minutia in minutiae_list:
            cylinder = self._create_3d_cylinder(central_minutia)

            neighbor_minutiae_in_range = []
            for other_minutiae in minutiae_list:
                if other_minutiae == central_minutia:
                    continue
                dist_sq = (central_minutia['x'] - other_minutiae['x'])**2 + \
                          (central_minutia['y'] - other_minutiae['y'])**2
                if dist_sq <= self.radius**2:
                    neighbor_minutiae_in_range.append(other_minutiae)

            cm_x, cm_y = central_minutia['x'], central_minutia['y']
            if not (0 <= cm_y < img_h and 0 <= cm_x < img_w):
                continue

            cm_orient = orientation_map[cm_y, cm_x]
            cm_freq = frequency_map[cm_y, cm_x]
            cm_energy = energy_map[cm_y, cm_x]


            for cell in cylinder['cells']:
                cell_x, cell_y = cell['center_x'], cell['center_y']

                if not (0 <= cell_y < img_h and 0 <= cell_x < img_w):
                    cell['contributions'] = {
                        'MCCo': 0.0, 'MCCf': 0.0, 'MCCe': 0.0,
                        'MCCco': 0.0, 'MCCcf': 0.0, 'MCCce': 0.0
                    }
                    continue

                sampled_orient_cell = orientation_map[cell_y, cell_x]
                sampled_freq_cell = frequency_map[cell_y, cell_x]
                sampled_energy_cell = energy_map[cell_y, cell_x]

                cell['contributions']['MCCo'] = 0.0
                cell['contributions']['MCCf'] = 0.0
                cell['contributions']['MCCe'] = 0.0
                cell['contributions']['MCCco'] = 0.0
                cell['contributions']['MCCcf'] = 0.0
                cell['contributions']['MCCce'] = 0.0

                for neighbor_minutia in neighbor_minutiae_in_range:
                    spatial_contrib = self._calculate_spatial_contribution(central_minutia, neighbor_minutia)

                    n_x, n_y = neighbor_minutia['x'], neighbor_minutia['y']
                    if not (0 <= n_y < img_h and 0 <= n_x < img_w):
                        continue

                    sampled_orient_neighbor = orientation_map[n_y, n_x]
                    sampled_freq_neighbor = frequency_map[n_y, n_x]
                    sampled_energy_neighbor = energy_map[n_y, n_x]

                    diff_orient_minutia = np.abs(central_minutia['orientation'] - neighbor_minutia['orientation'])
                    cell['contributions']['MCCo'] += spatial_contrib * self._calculate_texture_contribution(diff_orient_minutia, 0, self.sigma_d_orient)

                    freq_diff_central_neighbor = np.abs(cm_freq - sampled_freq_neighbor)
                    cell['contributions']['MCCf'] += spatial_contrib * self._calculate_texture_contribution(freq_diff_central_neighbor, 0, self.sigma_d_freq)

                    energy_diff_central_neighbor = np.abs(cm_energy - sampled_energy_neighbor)
                    cell['contributions']['MCCe'] += spatial_contrib * self._calculate_texture_contribution(energy_diff_central_neighbor, 0, self.sigma_d_energy)

                    orient_diff_cell_center = np.abs(central_minutia['orientation'] - sampled_orient_cell)
                    cell['contributions']['MCCco'] += spatial_contrib * self._calculate_texture_contribution(orient_diff_cell_center, 0, self.sigma_d_orient)

                    freq_diff_cell_center = np.abs(cm_freq - sampled_freq_cell)
                    cell['contributions']['MCCcf'] += spatial_contrib * self._calculate_texture_contribution(freq_diff_cell_center, 0, self.sigma_d_freq)

                    energy_diff_cell_center = np.abs(cm_energy - sampled_energy_cell)
                    cell['contributions']['MCCce'] += spatial_contrib * self._calculate_texture_contribution(energy_diff_cell_center, 0, self.sigma_d_energy)

            mtcc_descriptors.append(cylinder)

        return mtcc_descriptors

# -----------------------------------------------------------------------------
# 4. Matching Classes (Unchanged)
# -----------------------------------------------------------------------------

class MTCCMatcher:
    """
    Performs MTCC matching using Local Similarity Sort with Relaxation (LSSR).
    """
    def __init__(self, top_n_pairs: int = 20):
        self.top_n_pairs = top_n_pairs

    def _compute_local_similarity_matrix(self, cylinders1: list[dict], cylinders2: list[dict], feature_type: str) -> np.ndarray:
        n1 = len(cylinders1)
        n2 = len(cylinders2)
        similarity_matrix = np.zeros((n1, n2))

        for i, cyl1 in enumerate(cylinders1):
            for j, cyl2 in enumerate(cylinders2):
                if cyl1 is None or cyl2 is None or not cyl1.get('cells') or not cyl2.get('cells'):
                    similarity_matrix[i, j] = 0.0
                    continue

                cell_diff_sum = 0
                num_comparisons = 0
                for cell1 in cyl1['cells']:
                    for cell2 in cyl2['cells']:
                        if feature_type in cell1['contributions'] and feature_type in cell2['contributions']:
                            cell_diff_sum += np.abs(cell1['contributions'][feature_type] - cell2['contributions'][feature_type])
                            num_comparisons += 1

                if num_comparisons > 0:
                    average_diff = cell_diff_sum / num_comparisons
                    score = 1.0 / (average_diff + 1e-6)
                else:
                    score = 0.0

                similarity_matrix[i, j] = score
        return similarity_matrix

    def _select_top_matching_pairs(self, similarity_matrix: np.ndarray) -> list[dict]:
        flat_indices = np.argsort(similarity_matrix.flatten())[::-1]

        top_pairs = []
        matched_cyl1_indices = set()
        matched_cyl2_indices = set()

        for idx in flat_indices:
            r, c = np.unravel_index(idx, similarity_matrix.shape)
            if r not in matched_cyl1_indices and c not in matched_cyl2_indices and similarity_matrix[r, c] > 0:
                top_pairs.append({'cyl1_idx': r, 'cyl2_idx': c, 'score': similarity_matrix[r, c]})
                matched_cyl1_indices.add(r)
                matched_cyl2_indices.add(c)
                if len(top_pairs) >= self.top_n_pairs:
                    break
        return top_pairs

    def _apply_relaxation(self, top_pairs: list[dict]) -> list[float]:
        relaxed_scores = [p['score'] for p in top_pairs]
        return relaxed_scores

    def _compute_global_score(self, relaxed_scores: list[float]) -> float:
        if not relaxed_scores:
            return 0.0
        return np.mean(relaxed_scores)

    def match(self, cylinders1: list[dict], cylinders2: list[dict], feature_type: str = 'MCCco') -> float:
        if not cylinders1 or not cylinders2 or any(c is None for c in cylinders1) or any(c is None for c in cylinders2):
            return 0.0

        similarity_matrix = self._compute_local_similarity_matrix(cylinders1, cylinders2, feature_type)
        top_pairs = self._select_top_matching_pairs(similarity_matrix)

        relaxed_scores = self._apply_relaxation(top_pairs)

        final_score = self._compute_global_score(relaxed_scores)
        return final_score

# -----------------------------------------------------------------------------
# 5. Full Pipeline Orchestration Class (Modified)
# -----------------------------------------------------------------------------

class FingerprintRecognitionPipeline:
    """
    Orchestrates the entire fingerprint recognition pipeline from raw image
    to MTCC descriptor generation for a single image.
    """
    def __init__(self, params: MTCCParameters):
        self.params = params
        self.preprocessor = Preprocessor(params)
        self.stft_extractor = STFTFeatureExtractor(params)
        self.gabor_enhancer_wrapper = EnhancedGaborFilterWrapper(params)
        self.texture_extractor = TextureFeatureExtractor()
        self.minutiae_extractor = MinutiaeExtractor(params) # New minutiae extractor
        self.mtcc_generator = MTCCDescriptorGenerator(params)

    def process_image(self, image_path: str) -> dict:
        """
        Processes a single fingerprint image through the entire pipeline
        to generate MTCC descriptors and intermediate results.

        Args:
            image_path (str): Path to the input fingerprint image.

        Returns:
            dict: A dictionary containing all intermediate processing results
                  and the final MTCC descriptors.
        """
        # 1. Initial Preprocessing: Load, Normalize, Segment (for mask and STFT input)
        norm_img_for_stft, initial_mask, original_img = self.preprocessor.process(image_path)

        # 2. STFT-based Feature Extraction (for MTCC texture maps)
        stft_orientation_map, stft_frequency_map, stft_energy_map, stft_coherence_map = \
            self.stft_extractor.process(norm_img_for_stft)

        # 3. New Gabor-based Enhancement
        gabor_enhancer_output = self.gabor_enhancer_wrapper.process(original_img, invert_output=False)
        enhanced_binary_img = gabor_enhancer_output['enhanced_binary_img']
        
        # 4. Minutiae Extraction (includes skeletonization and filtering)
        minutiae_extraction_output = self.minutiae_extractor.process(enhanced_binary_img, initial_mask, stft_orientation_map)
        minutiae_list = minutiae_extraction_output['minutiae_list']
        thinned_skeleton = minutiae_extraction_output['thinned_skeleton']

        # 5. Texture Feature Extraction (uses STFT maps)
        texture_maps = self.texture_extractor.process(stft_orientation_map, stft_frequency_map, stft_energy_map)

        # 6. MTCC Descriptor Generation
        mtcc_cylinders = self.mtcc_generator.process(minutiae_list, texture_maps)

        return {
            "original_image": original_img,
            "preprocessed_img": norm_img_for_stft,
            "initial_mask": initial_mask,
            "stft_orientation_map": stft_orientation_map,
            "stft_frequency_map": stft_frequency_map,
            "stft_energy_map": stft_energy_map,
            "stft_coherence_map": stft_coherence_map,
            "gabor_enhanced_binary_img": enhanced_binary_img,
            "thinned_skeleton": thinned_skeleton, # From new minutiae extractor
            "minutiae_list": minutiae_list, # The MTCC-ready list
            "mtcc_cylinders": mtcc_cylinders,
            "enhancer_internal_mask": gabor_enhancer_output['mask'],
            "enhancer_internal_norm_img": gabor_enhancer_output['normalized_image'],
            "enhancer_internal_orient_map": gabor_enhancer_output['orientation_map'],
            "enhancer_internal_freq_map": gabor_enhancer_output['frequency_map'],
            "minutiaeTerm_img": minutiae_extraction_output['minutiaeTerm_img'], # From new minutiae extractor
            "minutiaeBif_img": minutiae_extraction_output['minutiaeBif_img'],   # From new minutiae extractor
            "enhancer_internal_mask_minutiae": minutiae_extraction_output['enhancer_internal_mask_minutiae'] # From new minutiae extractor's mask
        }

# -----------------------------------------------------------------------------
# 6. Evaluation Framework Classes (Unchanged)
# -----------------------------------------------------------------------------

class PerformanceEvaluator:
    """
    Handles calculation of EER and plotting of DET/ROC curves.
    """
    def calculate_eer(self, genuine_scores: list[float], impostor_scores: list[float]) -> float:
        if not genuine_scores and not impostor_scores:
            return 100.0

        scores = np.concatenate((genuine_scores, impostor_scores))
        labels = np.concatenate((np.ones(len(genuine_scores)), np.zeros(len(impostor_scores))))

        if len(scores) == 0:
            return 100.0

        sorted_indices = np.argsort(scores)
        scores = scores[sorted_indices]
        labels = labels[sorted_indices]

        far = []
        frr = []
        thresholds = np.unique(scores)

        for T in thresholds:
            fa = np.sum((scores >= T) & (labels == 0))
            tr = np.sum((scores >= T) & (labels == 1))
            fr = np.sum((scores < T) & (labels == 1))
            tn = np.sum((scores < T) & (labels == 0))

            current_far = fa / (fa + tn + 1e-6)
            current_frr = fr / (fr + tr + 1e-6)

            far.append(current_far)
            frr.append(current_frr)

        far = np.array(far)
        frr = np.array(frr)

        diffs = np.abs(far - frr)
        if len(diffs) == 0:
            return 100.0
        eer_idx = np.argmin(diffs)
        eer = (far[eer_idx] + frr[eer_idx]) / 2.0

        return eer * 100

    def plot_det_curves(self, results: dict):
        plt.figure(figsize=(10, 8))
        plt.title("DET Curves for MTCC Variants")
        plt.xlabel("False Acceptance Rate (FAR) (%)")
        plt.ylabel("False Rejection Rate (FRR) (%)")
        plt.grid(True)

        for method, data in results.items():
            if data['genuine'] and data['impostor']:
                scores = np.concatenate((data['genuine'], data['impostor']))
                labels = np.concatenate((np.ones(len(data['genuine'])), np.zeros(len(data['impostor']))))

                if len(scores) == 0:
                    print(f"Warning: No scores to plot DET curve for {method}.")
                    continue

                sorted_indices = np.argsort(scores)
                scores_sorted = scores[sorted_indices]
                labels_sorted = labels[sorted_indices]

                far_plot = []
                frr_plot = []
                thresholds = np.unique(scores_sorted)

                for T in thresholds:
                    fa = np.sum((scores_sorted >= T) & (labels_sorted == 0))
                    tr = np.sum((scores_sorted >= T) & (labels_sorted == 1))
                    fr = np.sum((scores_sorted < T) & (labels_sorted == 1))
                    tn = np.sum((scores_sorted < T) & (labels_sorted == 0))

                    current_far = fa / (fa + tn + 1e-6)
                    current_frr = fr / (fr + tr + 1e-6)

                    far_plot.append(current_far * 100)
                    frr_plot.append(current_frr * 100)

                plt.plot(far_plot, frr_plot, label=f"{method} (EER: {data['EER']:.2f}%)")
                plt.scatter(data['EER'], data['EER'], marker='x', s=100, color='red')
            else:
                print(f"Warning: Not enough scores to plot DET curve for {method}.")

        plt.legend()
        plt.xscale('log')
        plt.xlim(0.1, 100)
        plt.ylim(0.1, 100)
        plt.show()

    def plot_roc_curves(self, results: dict):
        plt.figure(figsize=(10, 8))
        plt.title("ROC Curves for MTCC Variants")
        plt.xlabel("False Acceptance Rate (FAR) (%)")
        plt.ylabel("Genuine Acceptance Rate (GAR) (%)")
        plt.grid(True)

        for method, data in results.items():
            if data['genuine'] and data['impostor']:
                scores = np.concatenate((data['genuine'], data['impostor']))
                labels = np.concatenate((np.ones(len(data['genuine'])), np.zeros(len(data['impostor']))))

                if len(scores) == 0:
                    print(f"Warning: No scores to plot ROC curve for {method}.")
                    continue

                sorted_indices = np.argsort(scores)
                scores_sorted = scores[sorted_indices]
                labels_sorted = labels[sorted_indices]

                far_plot = []
                gar_plot = []
                thresholds = np.unique(scores_sorted)

                for T in thresholds:
                    fa = np.sum((scores_sorted >= T) & (labels_sorted == 0))
                    tr = np.sum((scores_sorted >= T) & (labels_sorted == 1))
                    fr = np.sum((scores_sorted < T) & (labels_sorted == 1))
                    tn = np.sum((scores_sorted < T) & (labels_sorted == 0))

                    current_far = fa / (fa + tn + 1e-6)
                    current_gar = tr / (fr + tr + 1e-6)

                    far_plot.append(current_far * 100)
                    gar_plot.append(current_gar * 100)

                plt.plot(far_plot, gar_plot, label=f"{method} (EER: {data['EER']:.2f}%)")
                plt.scatter(data['EER'], 100 - data['EER'], marker='x', s=100, color='red')
            else:
                print(f"Warning: Not enough scores to plot ROC curve for {method}.")

        plt.legend()
        plt.xscale('log')
        plt.xlim(0.1, 100)
        plt.ylim(0, 100)
        plt.show()

    def generate_performance_table(self, results: dict):
        print("\n--- Performance Benchmarks (Based on Actual Descriptor Matching) ---")
        print(f"{'Method':<10} | {'EER (%)':<10}")
        print("-" * 23)
        for method, data in results.items():
            print(f"{method:<10} | {data['EER']:<10.2f}")
        print("-" * 23)
        print("\nNote: The accuracy of the EER depends on the completeness and robustness of conceptual components (e.g., minutiae extraction, texture map accuracy, matching logic).")

class FVCDatasetEvaluator:
    """
    Manages the evaluation protocol for FVC datasets, including processing images,
    generating descriptors, performing matches, and calculating performance metrics.
    """
    def __init__(self, params: MTCCParameters):
        self.params = params
        self.pipeline = FingerprintRecognitionPipeline(params)
        self.matcher = MTCCMatcher(top_n_pairs=params.matcher_top_n_pairs)
        self.evaluator = PerformanceEvaluator()

    def evaluate(self, dataset_path: str, num_users: int = 100, images_per_user: int = 8, methods_to_test: list = None) -> dict:
        if methods_to_test is None:
            methods_to_test = ['MCCco']

        print(f"Starting FVC dataset evaluation for {methods_to_test}...")

        all_cylinders = [[] for _ in range(num_users)]

        print("Generating MTCC descriptors for dataset...")
        for finger_idx in range(num_users):
            current_finger_id = finger_idx + 1
            finger_id_str = str(current_finger_id)

            for impression_idx in range(images_per_user):
                current_impression_id = impression_idx + 1
                impression_id_str = str(current_impression_id)

                img_filename = f"{finger_id_str}_{impression_id_str}.tif"
                img_path = os.path.join(dataset_path, img_filename)

                if not os.path.exists(img_path):
                    print(f"Warning: Image not found at {img_path}. Skipping this impression.")
                    all_cylinders[finger_idx].append(None)
                    continue

                try:
                    pipeline_output = self.pipeline.process_image(img_path)
                    cylinders = pipeline_output['mtcc_cylinders']
                    all_cylinders[finger_idx].append(cylinders)
                except FileNotFoundError as e:
                    print(f"Error processing {img_path}: {e}")
                    all_cylinders[finger_idx].append(None)
                except Exception as e:
                    print(f"An unexpected error occurred during processing {img_path}: {e}")
                    all_cylinders[finger_idx].append(None)
        print("MTCC descriptors generation complete.")

        simulated_results = {}
        for method in methods_to_test:
            genuine_scores = []
            impostor_scores = []

            print(f"Starting matching process for {method}...")
            for u_idx in range(num_users):
                for i_idx in range(images_per_user):
                    for j_idx in range(i_idx + 1, images_per_user):
                        cyl1 = all_cylinders[u_idx][i_idx]
                        cyl2 = all_cylinders[u_idx][j_idx]

                        if cyl1 is not None and cyl2 is not None:
                            score = self.matcher.match(cyl1, cyl2, feature_type=method)
                            genuine_scores.append(score)

            for u1_idx in range(num_users):
                for u2_idx in range(u1_idx + 1, num_users):
                    cyl1 = all_cylinders[u1_idx][0]
                    cyl2 = all_cylinders[u2_idx][0]

                    if cyl1 is not None and cyl2 is not None:
                        score = self.matcher.match(cyl1, cyl2, feature_type=method)
                        impostor_scores.append(score)
            print("Matching complete.")

            eer = self.evaluator.calculate_eer(genuine_scores, impostor_scores)
            simulated_results[method] = {'EER': eer, 'genuine': genuine_scores, 'impostor': impostor_scores}
            print(f"Computed {method} EER: {eer:.2f}%")

        self.evaluator.plot_det_curves(simulated_results)
        self.evaluator.plot_roc_curves(simulated_results)
        self.evaluator.generate_performance_table(simulated_results)
        return simulated_results

# -----------------------------------------------------------------------------
# 7. Visualization Class (Modified to reflect new pipeline outputs)
# -----------------------------------------------------------------------------

class MTCCPipelineVisualizer:
    """
    Provides comprehensive visualization of the MTCC pipeline's intermediate steps.
    Updated to reflect the outputs of the new Gabor enhancement and minutiae extraction.
    """
    def visualize(self, pipeline_output: dict, save_debug: bool = True):
        print("Visualizing MTCC pipeline...")
        try:
            original_img = pipeline_output.get('original_image')
            if original_img is None:
                print("Error: Original image not found in pipeline output. Skipping visualization.")
                return

            fig, axes = plt.subplots(3, 4, figsize=(18, 12))
            fig.suptitle("MTCC Fingerprint Recognition Pipeline Visualization", fontsize=16)

            # Row 1: Processing Stages
            axes[0, 0].imshow(original_img, cmap='gray'); axes[0, 0].set_title("1. Original Image")
            axes[0, 1].imshow(pipeline_output['initial_mask'], cmap='gray'); axes[0, 1].set_title("2. Initial Mask (Preprocessor)")
            axes[0, 2].imshow(pipeline_output['gabor_enhanced_binary_img'], cmap='gray'); axes[0, 2].set_title("3. Gabor Enhanced Binary")
            axes[0, 3].imshow(pipeline_output['thinned_skeleton'], cmap='gray'); axes[0, 3].set_title("4. Thinned Skeleton (Minutiae Extractor)")

            # Row 2: Feature Maps (from STFT, used for MTCC descriptors) - CHANGED TO GRAYSCALE
            # Note: Orientation map typically looks better with 'hsv' but changed to 'gray' as requested.
            axes[1, 0].imshow(pipeline_output['stft_orientation_map'], cmap='gray'); axes[1, 0].set_title("5. STFT Orientation Map (Grayscale)")
            axes[1, 1].imshow(pipeline_output['stft_frequency_map'], cmap='gray'); axes[1, 1].set_title("6. STFT Frequency Map (Grayscale)")
            axes[1, 2].imshow(pipeline_output['stft_energy_map'], cmap='gray'); axes[1, 2].set_title("7. STFT Energy Map (Grayscale)")
            axes[1, 3].imshow(pipeline_output['stft_coherence_map'], cmap='gray'); axes[1, 3].set_title("8. STFT Coherence Map (Grayscale)")

            # Row 3: Minutiae Visualization & Enhancer's Internal Maps
            minutiae_img_colored = cv2.cvtColor(pipeline_output['thinned_skeleton'].copy(), cv2.COLOR_GRAY2BGR)

            minutiae_list = pipeline_output['minutiae_list']
            for m in minutiae_list:
                # Plot minutiae points
                center_x, center_y = m['x'], m['y']
                color = (0, 0, 255) if m['type'] == 'ending' else (255, 0, 0) # Red for ending, Blue for bifurcation
                cv2.circle(minutiae_img_colored, (center_x, center_y), 3, color, -1)
                
                # Plot orientation as a line
                length = 10 # Length of orientation line
                # Note: Orientation is in radians, angle is measured from positive x-axis counter-clockwise.
                # In image coordinates, y-axis is inverted, so sin needs negation or adjust angle.
                # Standard convention for orientation map in fingerprint is 0-pi, where 0 is horizontal right.
                # For plotting, angle 0 is towards right, pi/2 is down, pi is left.
                # Our orientation map from STFT ranges 0 to pi.
                # For `np.cos(angle)` and `np.sin(angle)` where angle is from horizontal right,
                # `end_x = center_x + length * np.cos(angle)`
                # `end_y = center_y + length * np.sin(angle)`
                # However, in image coordinates positive Y is usually downwards, while mathematical Y is upwards.
                # To make the orientation vector point *along* the ridge flow visually:
                # Ridge orientation is perpendicular to gradient direction. `stft_orientation_map` is ridge orientation.
                # If 0 is horizontal, pi/2 is vertical, plot points in direction:
                end_x = int(center_x + length * np.cos(m['orientation']))
                end_y = int(center_y + length * np.sin(m['orientation'])) # Using + for y makes it point downwards if angle is 0 to pi
                cv2.line(minutiae_img_colored, (center_x, center_y), (end_x, end_y), (0, 255, 0), 1) # Green line for orientation

            axes[2, 0].imshow(cv2.cvtColor(minutiae_img_colored, cv2.COLOR_BGR2RGB)); axes[2, 0].set_title(f"9. Final Minutiae ({len(minutiae_list)} found)")

            # Internal maps from the Gabor Enhancer
            enhancer_norm_for_plot = pipeline_output['enhancer_internal_norm_img']
            axes[2, 1].imshow(enhancer_norm_for_plot, cmap='gray'); axes[2, 1].set_title("10. Gabor Enhancer's Normalized Image")

            enhancer_orient_for_plot = (pipeline_output['enhancer_internal_orient_map'] - np.min(pipeline_output['enhancer_internal_orient_map'])) / (np.max(pipeline_output['enhancer_internal_orient_map']) - np.min(pipeline_output['enhancer_internal_orient_map']) + 1e-6)
            axes[2, 2].imshow(enhancer_orient_for_plot, cmap='gray'); axes[2, 2].set_title("11. Gabor Enhancer's Orient Map (Grayscale)")

            axes[2, 3].imshow(pipeline_output['enhancer_internal_freq_map'], cmap='gray'); axes[2, 3].set_title("12. Gabor Enhancer's Freq Map (Grayscale)")


            for ax_row in axes:
                for ax in ax_row:
                    ax.set_xticks([])
                    ax.set_yticks([])

            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            if save_debug:
                plt.savefig("mtcc_pipeline_visualization_refactored_new_minutiae.png")
            plt.show()

        except Exception as e:
            print(f"Error during visualization: {e}")
            import traceback
            traceback.print_exc()

# -----------------------------------------------------------------------------
# Function to match two images and print score
# -----------------------------------------------------------------------------
def match_two_fingerprints(image_path1: str, image_path2: str, params: MTCCParameters, feature_type: str = 'MCCco'):
    """
    Processes two fingerprint images and computes their MTCC matching score.
    """
    print(f"\n--- Matching {os.path.basename(image_path1)} and {os.path.basename(image_path2)} ---")
    pipeline = FingerprintRecognitionPipeline(params)
    matcher = MTCCMatcher(top_n_pairs=params.matcher_top_n_pairs)

    try:
        print(f"Processing {os.path.basename(image_path1)}...")
        output1 = pipeline.process_image(image_path1)
        cylinders1 = output1['mtcc_cylinders']
        if not cylinders1:
            print(f"Warning: No MTCC descriptors found for {os.path.basename(image_path1)}. Score will be 0.")
            print(f"Match Score ({feature_type}): 0.0")
            return 0.0

        print(f"Processing {os.path.basename(image_path2)}...")
        output2 = pipeline.process_image(image_path2)
        cylinders2 = output2['mtcc_cylinders']
        if not cylinders2:
            print(f"Warning: No MTCC descriptors found for {os.path.basename(image_path2)}. Score will be 0.")
            print(f"Match Score ({feature_type}): 0.0")
            return 0.0

        score = matcher.match(cylinders1, cylinders2, feature_type=feature_type)
        print(f"Match Score ({feature_type}): {score:.4f}")
        return score
    except FileNotFoundError as e:
        print(f"Error: {e}")
        return 0.0
    except Exception as e:
        print(f"An unexpected error occurred during matching: {e}")
        import traceback
        traceback.print_exc()
        return 0.0


# -----------------------------------------------------------------------------
# Main Execution Block
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    params = MTCCParameters()

    # --- 1. Visualization of the pipeline (requires an actual image file) ---
    # IMPORTANT: Update this path to a real fingerprint image from your FVC2002 dataset.
    sample_image_path = 'C:/Users/Precision/Onus/Data/FVC-DataSets/DataSets/FVC2002/Db1_a/1_1.tif'

    print("--- Starting Visualization Test ---")
    if not os.path.exists(sample_image_path):
        print(f"Error: Visualization image not found at '{sample_image_path}'.")
        print("Please update 'sample_image_path' to a valid fingerprint image to run visualization.")
    else:
        pipeline = FingerprintRecognitionPipeline(params)
        visualizer = MTCCPipelineVisualizer()
        try:
            pipeline_output = pipeline.process_image(sample_image_path)
            visualizer.visualize(pipeline_output, save_debug=True)
        except Exception as e:
            print(f"Failed to run visualization pipeline: {e}")
            import traceback
            traceback.print_exc()
    print("--- Visualization Test Complete ---")


    # --- 2. Single Image Match Test ---
    # IMPORTANT: Update these paths to real fingerprint images from your FVC2002 dataset.
    # Ensure these paths are correct for your system.
    print("\n--- Starting Single Image Match Test ---")
    
    # Example 1: Genuine Match (same finger, different impressions)
    image_path_genuine_1 = 'C:/Users/Precision/Onus/Data/FVC-DataSets/DataSets/FVC2002/Db1_a/1_1.tif'
    image_path_genuine_2 = 'C:/Users/Precision/Onus/Data/FVC-DataSets/DataSets/FVC2002/Db1_a/1_2.tif'

    # Example 2: Impostor Match (different fingers)
    image_path_impostor_1 = 'C:/Users/Precision/Onus/Data/FVC-DataSets/DataSets/FVC2002/Db1_a/1_1.tif'
    image_path_impostor_2 = 'C:/Users/Precision/Onus/Data/FVC-DataSets/DataSets/FVC2002/Db1_a/2_1.tif'

    # Check if paths exist before running tests
    all_paths_exist = True
    for path in [image_path_genuine_1, image_path_genuine_2, image_path_impostor_1, image_path_impostor_2]:
        if not os.path.exists(path):
            print(f"Error: Required image not found at '{path}'. Skipping single match test.")
            all_paths_exist = False
            break

    if all_paths_exist:
        # Perform Genuine match
        match_two_fingerprints(image_path_genuine_1, image_path_genuine_2, params)
        # Perform Impostor match
        match_two_fingerprints(image_path_impostor_1, image_path_impostor_2, params)
    print("--- Single Image Match Test Complete ---")


    # --- 3. FVC Dataset Evaluation ---
    # IMPORTANT: Update this path to your actual FVC2002 Db1_a directory.
    dataset_path = 'C:/Users/Precision/Onus/Data/FVC-DataSets/DataSets/FVC2002/Db1_a'

    methods_to_test = ['MCCo', 'MCCf', 'MCCe', 'MCCco', 'MCCcf', 'MCCce']

    print("\n--- Starting FVC Dataset Evaluation ---")
    if not os.path.exists(dataset_path):
        print(f"Error: FVC dataset path not found at '{dataset_path}'.")
        print("Please update 'dataset_path' to a valid FVC2002 Db1_a directory to run the evaluation.")
    else:
        fvc_evaluator = FVCDatasetEvaluator(params)
        fvc_evaluator.evaluate(dataset_path, num_users=100, images_per_user=8, methods_to_test=methods_to_test)
    print("--- FVC Dataset Evaluation Complete ---")